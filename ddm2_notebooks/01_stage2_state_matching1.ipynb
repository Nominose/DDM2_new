{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDM² Stage 2: State Matching\n",
    "\n",
    "为每个训练样本找到最优的扩散时间步 t，使得 `x_t = √ᾱ_t * teacher + √(1-ᾱ_t) * noise` 最接近原始噪声图像。\n",
    "\n",
    "修改下面的配置，然后运行所有cell。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# 基础路径配置\n",
    "#==============================================================================\n",
    "\n",
    "PROJECT_ROOT = \"/host/c/Users/ROG/Documents/Github/DDM2_new\"          # 项目根目录\n",
    "CONFIG_FILE = \"config/ct_denoise.json\" # 配置文件路径 (相对于PROJECT_ROOT)\n",
    "STAGE2_EXP_NAME = \"ct_denoise_teacher\" # Stage2输出目录名，结果保存在 experiments/{STAGE2_EXP_NAME}/\n",
    "GPU_ID = \"0\"                           # 使用的GPU编号\n",
    "\n",
    "#==============================================================================\n",
    "# 数据路径\n",
    "#==============================================================================\n",
    "\n",
    "EXCEL_PATH = \"/host/d/file/fixedCT_static_simulation_train_test_gaussian_local.xlsx\"  # 数据索引Excel文件\n",
    "DATA_ROOT = \"/host/d/file/simulation/\"                    # CT数据根目录\n",
    "TEACHER_N2N_ROOT = \"/host/d/file/pre/noise2noise/pred_images/\"  # Teacher N2N预测结果目录\n",
    "TEACHER_N2N_EPOCH = 78                                    # 使用Teacher N2N的哪个epoch\n",
    "BINS_FILE = \"/host/d/file/histogram_equalization/bins.npy\"        # 直方图均衡化bins文件\n",
    "BINS_MAPPED_FILE = \"/host/d/file/histogram_equalization/bins_mapped.npy\"  # 直方图均衡化映射文件\n",
    "\n",
    "#==============================================================================\n",
    "# 数据集划分\n",
    "# \n",
    "# 数据组织结构:\n",
    "#   - batch: 一组病例 (例如batch 0-4为训练集，batch 5为验证集)\n",
    "#   - volume: 一个病例/case，即一个完整的3D CT扫描\n",
    "#   - slice: volume中的一个2D切片\n",
    "#\n",
    "# 例如: volume_idx=3, slice_idx=45 表示第3个病例的第45层切片\n",
    "#==============================================================================\n",
    "\n",
    "TRAIN_BATCHES = [0, 1, 2, 3, 4]  # 训练集使用的batch编号\n",
    "VAL_BATCHES = [5]                # 验证集使用的batch编号\n",
    "\n",
    "SLICE_RANGE = [30, 80]  # 每个volume使用的切片范围 [start, end)\n",
    "                        # 例如[30, 80]表示使用第30-79层，共50层\n",
    "                        # 通常去掉头尾切片，因为边缘切片质量较差\n",
    "\n",
    "# 验证集采样设置 (Stage2时建议用 'all' 匹配所有样本)\n",
    "VAL_VOLUME_IDX = \"all\"  # 验证哪些volume: 'all'=所有, 或具体数字如 8 表示只用第8个case\n",
    "VAL_SLICE_IDX = \"all\"   # 验证哪些slice: 'all'=所有, 或列表如 [25, 30] 表示只用这些层\n",
    "\n",
    "#==============================================================================\n",
    "# 预处理参数\n",
    "#==============================================================================\n",
    "\n",
    "HU_MIN = -1000.0  # CT值(HU)下限，低于此值会被clip\n",
    "HU_MAX = 2000.0   # CT值(HU)上限，高于此值会被clip\n",
    "                  # 典型值: 空气=-1000, 水=0, 骨骼=1000+\n",
    "                  # 范围[-1000, 2000]覆盖大部分软组织和骨骼\n",
    "\n",
    "HISTOGRAM_EQUALIZATION = True  # 是否启用直方图均衡化\n",
    "                               # True: 增强对比度，让网络更容易学习\n",
    "                               # False: 保持原始HU分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config已更新: config/ct_denoise.json\n",
      "输出目录: experiments/ct_denoise_teacher\n",
      "Train batches: [0, 1, 2, 3, 4], Val batches: [5]\n",
      "Slice range: [30, 80], HU: [-1000.0, 2000.0]\n"
     ]
    }
   ],
   "source": [
    "# 准备工作：更新config + 创建目录\n",
    "import os, json\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# 更新 datasets.train\n",
    "config['datasets']['train']['dataroot'] = EXCEL_PATH\n",
    "config['datasets']['train']['data_root'] = DATA_ROOT\n",
    "config['datasets']['train']['teacher_n2n_root'] = TEACHER_N2N_ROOT\n",
    "config['datasets']['train']['teacher_n2n_epoch'] = TEACHER_N2N_EPOCH\n",
    "config['datasets']['train']['bins_file'] = BINS_FILE\n",
    "config['datasets']['train']['bins_mapped_file'] = BINS_MAPPED_FILE\n",
    "config['datasets']['train']['train_batches'] = TRAIN_BATCHES\n",
    "config['datasets']['train']['val_batches'] = VAL_BATCHES\n",
    "config['datasets']['train']['slice_range'] = SLICE_RANGE\n",
    "config['datasets']['train']['HU_MIN'] = HU_MIN\n",
    "config['datasets']['train']['HU_MAX'] = HU_MAX\n",
    "config['datasets']['train']['histogram_equalization'] = HISTOGRAM_EQUALIZATION\n",
    "\n",
    "# 更新 datasets.val\n",
    "config['datasets']['val']['dataroot'] = EXCEL_PATH\n",
    "config['datasets']['val']['data_root'] = DATA_ROOT\n",
    "config['datasets']['val']['teacher_n2n_root'] = TEACHER_N2N_ROOT\n",
    "config['datasets']['val']['teacher_n2n_epoch'] = TEACHER_N2N_EPOCH\n",
    "config['datasets']['val']['bins_file'] = BINS_FILE\n",
    "config['datasets']['val']['bins_mapped_file'] = BINS_MAPPED_FILE\n",
    "config['datasets']['val']['train_batches'] = VAL_BATCHES  # val用val_batches\n",
    "config['datasets']['val']['val_batches'] = VAL_BATCHES\n",
    "config['datasets']['val']['slice_range'] = SLICE_RANGE\n",
    "config['datasets']['val']['val_volume_idx'] = VAL_VOLUME_IDX\n",
    "config['datasets']['val']['val_slice_idx'] = VAL_SLICE_IDX\n",
    "config['datasets']['val']['HU_MIN'] = HU_MIN\n",
    "config['datasets']['val']['HU_MAX'] = HU_MAX\n",
    "config['datasets']['val']['histogram_equalization'] = HISTOGRAM_EQUALIZATION\n",
    "\n",
    "# 更新 stage2_file 路径\n",
    "config['stage2_file'] = f\"experiments/{STAGE2_EXP_NAME}/stage2_matched.txt\"\n",
    "\n",
    "with open(CONFIG_FILE, 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(f\"experiments/{STAGE2_EXP_NAME}\", exist_ok=True)\n",
    "\n",
    "print(f\"Config已更新: {CONFIG_FILE}\")\n",
    "print(f\"输出目录: experiments/{STAGE2_EXP_NAME}\")\n",
    "print(f\"Train batches: {TRAIN_BATCHES}, Val batches: {VAL_BATCHES}\")\n",
    "print(f\"Slice range: {SLICE_RANGE}, HU: [{HU_MIN}, {HU_MAX}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "26-01-03 12:52:26.722 - INFO: [Stage 2] Markov chain state matching (using teacher N2N)!\n",
      "[train] Histogram equalization enabled:\n",
      "    bins: /host/d/file/histogram_equalization/bins.npy (shape: (2301,))\n",
      "    bins_mapped: /host/d/file/histogram_equalization/bins_mapped.npy (shape: (2301,))\n",
      "Found 69 N2N pairs\n",
      "[Slice Detection] Noise: 100 slices, Teacher: 50 slices\n",
      "[Slice Detection] Best offset: 30, correlation: 0.9998\n",
      "[OK] Slice offset verified: 30\n",
      "[train] CTDataset: pairs=69, slices=50, samples=3450\n",
      "[train] Noise slice_range: [30, 80)\n",
      "[train] HU range: [-1000.0, 2000.0]\n",
      "[train] Histogram equalization: True\n",
      "[train] Using teacher N2N from: /host/d/file/pre/noise2noise/pred_images/\n",
      "26-01-03 12:52:32.871 - INFO: CT dataset [ct] is created. Size: 3450\n",
      "[val] Histogram equalization enabled:\n",
      "    bins: /host/d/file/histogram_equalization/bins.npy (shape: (2301,))\n",
      "    bins_mapped: /host/d/file/histogram_equalization/bins_mapped.npy (shape: (2301,))\n",
      "Found 14 N2N pairs\n",
      "[Slice Detection] Noise: 100 slices, Teacher: 50 slices\n",
      "[Slice Detection] Best offset: 30, correlation: 0.9998\n",
      "[OK] Slice offset verified: 30\n",
      "[val] CTDataset: pairs=14, slices=50, samples=50\n",
      "[val] Noise slice_range: [30, 80)\n",
      "[val] HU range: [-1000.0, 2000.0]\n",
      "[val] Histogram equalization: True\n",
      "[val] Using teacher N2N from: /host/d/file/pre/noise2noise/pred_images/\n",
      "26-01-03 12:52:36.871 - INFO: CT dataset [ct] is created. Size: 50\n",
      "26-01-03 12:52:36.872 - INFO: Initial Dataset Finished\n",
      "26-01-03 12:52:36.873 - INFO: Using teacher N2N results from ct_dataset\n",
      "26-01-03 12:52:37.773 - INFO: Matching train set (3450 samples)...\n",
      "Train: 100%|████████████████████████████████| 3450/3450 [02:38<00:00, 21.82it/s]\n",
      "26-01-03 12:55:16.010 - INFO: Matching val set (50 samples)...\n",
      "Val: 100%|██████████████████████████████████████| 50/50 [00:09<00:00,  5.10it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# 运行State Matching (train + val)\n",
    "# 输出文件格式: volume_idx_slice_idx_t (每行一个样本)\n",
    "!CUDA_VISIBLE_DEVICES={GPU_ID} python3 match_state.py -p all -c {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Stage2完成: experiments/ct_denoise_teacher/stage2_matched.txt\n",
      "  总样本数: 3500\n",
      "  t值范围: [16, 72]\n",
      "  t值均值: 33.4\n",
      "\n",
      "  t < 100 (低噪声): 3500 样本\n",
      "  100 <= t < 300: 0 样本\n",
      "  t >= 300 (高噪声): 0 样本\n"
     ]
    }
   ],
   "source": [
    "# 验证结果 + 统计t值分布\n",
    "import glob\n",
    "\n",
    "stage2_file = f\"experiments/{STAGE2_EXP_NAME}/stage2_matched.txt\"\n",
    "\n",
    "if os.path.exists(stage2_file):\n",
    "    with open(stage2_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(f\"✓ Stage2完成: {stage2_file}\")\n",
    "    print(f\"  总样本数: {len(lines)}\")\n",
    "    \n",
    "    # 统计t值分布\n",
    "    t_values = [int(line.strip().split('_')[-1]) for line in lines]\n",
    "    print(f\"  t值范围: [{min(t_values)}, {max(t_values)}]\")\n",
    "    print(f\"  t值均值: {sum(t_values)/len(t_values):.1f}\")\n",
    "    \n",
    "    # t值越大说明噪声越多，需要更多去噪步骤\n",
    "    print(f\"\\n  t < 100 (低噪声): {sum(1 for t in t_values if t < 100)} 样本\")\n",
    "    print(f\"  100 <= t < 300: {sum(1 for t in t_values if 100 <= t < 300)} 样本\")\n",
    "    print(f\"  t >= 300 (高噪声): {sum(1 for t in t_values if t >= 300)} 样本\")\n",
    "else:\n",
    "    print(f\"⚠ 未找到stage2文件: {stage2_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
