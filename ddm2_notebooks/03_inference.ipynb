{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDM² : Inference\n",
    "\n",
    "使用训练好的模型对测试集进行推理，生成去噪后的 CT 图像。\n",
    "\n",
    "**输出**：\n",
    "- `noisy_input.nii.gz` - 原始噪声输入\n",
    "- `n2n_teacher.nii.gz` - Teacher N2N 结果\n",
    "- `ddm2_first_step.nii.gz` - DDM² 第一步去噪\n",
    "- `ddm2_final.nii.gz` - DDM² 最终去噪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# 基础路径配置\n",
    "#==============================================================================\n",
    "\n",
    "PROJECT_ROOT = \"/host/c/Users/ROG/Documents/Github/DDM2_new\"          # 项目根目录\n",
    "CONFIG_FILE = \"config/ct_denoise.json\" # 配置文件路径\n",
    "GPU_ID = \"0\"                           # 使用的GPU编号\n",
    "\n",
    "#==============================================================================\n",
    "# 模型 Checkpoint\n",
    "#==============================================================================\n",
    "\n",
    "# 设为 None 自动查找最新的 checkpoint\n",
    "# 或指定具体路径，如 \"experiments/ct_denoise_241231_120000/checkpoint/latest\"\n",
    "CHECKPOINT = None\n",
    "\n",
    "#==============================================================================\n",
    "# 推理设置\n",
    "#==============================================================================\n",
    "\n",
    "# 推理模式\n",
    "INFERENCE_MODE = \"batch\"  # \"single\" = 单个患者, \"batch\" = 批量所有患者\n",
    "\n",
    "# 单个患者模式时的患者索引 (volume_idx)\n",
    "PATIENT_IDX = 0\n",
    "\n",
    "# 输出目录\n",
    "OUTPUT_DIR = \"/host/d/file/pre/ddm2/pred_images\"\n",
    "\n",
    "# 保存哪些结果 \n",
    "SAVE_NOISY = True        # 原始噪声输入\n",
    "SAVE_N2N = True          # Teacher N2N 结果\n",
    "SAVE_FIRST_STEP = True   # DDM² 第一步去噪结果\n",
    "SAVE_FINAL = True        # DDM² 最终去噪结果\n",
    "\n",
    "#==============================================================================\n",
    "# 后处理设置\n",
    "#==============================================================================\n",
    "\n",
    "# 是否进行逆向直方图均衡化，将结果转换回原始 HU 空间\n",
    "# True:  输出为原始 HU 值 (与输入 CT 在同一空间，可直接对比)\n",
    "# False: 输出为 HE 空间的 HU 值 (与训练时的空间一致)\n",
    "INVERSE_HE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化环境\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import nibabel as nib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import data as Data\n",
    "import model as Model\n",
    "import core.logger as Logger\n",
    "\n",
    "print(f\"工作目录: {os.getcwd()}\")\n",
    "print(f\"GPU: {GPU_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数\n",
    "\n",
    "def find_latest_checkpoint(experiments_dir='experiments'):\n",
    "    \"\"\"自动查找最新的 checkpoint\"\"\"\n",
    "    latest_dir = None\n",
    "    latest_time = 0\n",
    "    \n",
    "    if not os.path.exists(experiments_dir):\n",
    "        return None\n",
    "    \n",
    "    for d in os.listdir(experiments_dir):\n",
    "        if d.startswith('ct_denoise'):\n",
    "            ckpt_path = os.path.join(experiments_dir, d, 'checkpoint', 'latest_gen.pth')\n",
    "            if os.path.exists(ckpt_path):\n",
    "                mtime = os.path.getmtime(ckpt_path)\n",
    "                if mtime > latest_time:\n",
    "                    latest_time = mtime\n",
    "                    latest_dir = os.path.join(experiments_dir, d, 'checkpoint', 'latest')\n",
    "    \n",
    "    return latest_dir\n",
    "\n",
    "\n",
    "def inverse_histogram_equalization(img, bins, bins_mapped):\n",
    "    \"\"\"\n",
    "    逆向 HE：HE 空间 → 原始 HU\n",
    "    \"\"\"\n",
    "    if bins is None or bins_mapped is None:\n",
    "        return img\n",
    "    \n",
    "    flat_img = img.flatten()\n",
    "    bin_indices = np.digitize(flat_img, bins_mapped) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, len(bins) - 1)\n",
    "    original = bins[bin_indices]\n",
    "    \n",
    "    return original.reshape(img.shape).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载配置和准备\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    opt = json.load(f)\n",
    "\n",
    "HU_MIN = opt['datasets']['val'].get('HU_MIN', -1000.0)\n",
    "HU_MAX = opt['datasets']['val'].get('HU_MAX', 2000.0)\n",
    "\n",
    "# Teacher N2N 路径\n",
    "TEACHER_N2N_ROOT = opt['datasets']['val'].get('teacher_n2n_root')\n",
    "TEACHER_N2N_EPOCH = opt['datasets']['val'].get('teacher_n2n_epoch', 78)\n",
    "\n",
    "# 加载 HE bins (用于逆向转换)\n",
    "bins_file = opt['datasets']['val'].get('bins_file')\n",
    "bins_mapped_file = opt['datasets']['val'].get('bins_mapped_file')\n",
    "bins = None\n",
    "bins_mapped = None\n",
    "use_inverse_he = False\n",
    "\n",
    "if INVERSE_HE and bins_file and bins_mapped_file:\n",
    "    if os.path.exists(bins_file) and os.path.exists(bins_mapped_file):\n",
    "        bins = np.load(bins_file).astype(np.float32)\n",
    "        bins_mapped = np.load(bins_mapped_file).astype(np.float32)\n",
    "        use_inverse_he = True\n",
    "        print(f\"✓ HE bins 已加载\")\n",
    "    else:\n",
    "        print(f\"⚠ HE bins 文件不存在，将跳过逆向 HE\")\n",
    "\n",
    "# 查找 checkpoint\n",
    "checkpoint = CHECKPOINT\n",
    "if checkpoint is None:\n",
    "    checkpoint = find_latest_checkpoint()\n",
    "\n",
    "if checkpoint is None:\n",
    "    print(\"❌ 未找到 checkpoint！\")\n",
    "else:\n",
    "    print(f\"✓ Checkpoint: {checkpoint}\")\n",
    "\n",
    "print(f\"\\n配置:\")\n",
    "print(f\"  HU 范围: [{HU_MIN}, {HU_MAX}]\")\n",
    "print(f\"  逆向 HE: {use_inverse_he}\")\n",
    "print(f\"  Teacher N2N: {TEACHER_N2N_ROOT}\")\n",
    "print(f\"  推理模式: {INFERENCE_MODE}\")\n",
    "print(f\"  输出目录: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "print(\"加载数据集...\")\n",
    "\n",
    "val_opt = opt['datasets']['val'].copy()\n",
    "\n",
    "if INFERENCE_MODE == \"single\":\n",
    "    val_opt['val_volume_idx'] = PATIENT_IDX\n",
    "else:\n",
    "    val_opt['val_volume_idx'] = 'all'\n",
    "\n",
    "val_opt['val_slice_idx'] = 'all'\n",
    "\n",
    "val_set = Data.create_dataset(val_opt, 'val', stage2_file=opt.get('stage2_file'))\n",
    "\n",
    "# 获取患者列表\n",
    "if hasattr(val_set, 'n2n_pairs'):\n",
    "    n_patients = len(val_set.n2n_pairs)\n",
    "    if INFERENCE_MODE == \"single\":\n",
    "        patient_indices = [PATIENT_IDX] if PATIENT_IDX < n_patients else []\n",
    "    else:\n",
    "        patient_indices = list(range(n_patients))\n",
    "else:\n",
    "    patient_indices = list(set(s[0] for s in val_set.samples))\n",
    "\n",
    "print(f\"\\n✓ 数据集加载完成\")\n",
    "print(f\"  患者数量: {len(patient_indices)}\")\n",
    "print(f\"  总样本数: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "print(\"加载模型...\")\n",
    "\n",
    "opt_model = Logger.dict_to_nonedict(opt)\n",
    "opt_model['path']['resume_state'] = checkpoint\n",
    "\n",
    "diffusion = Model.create_model(opt_model)\n",
    "diffusion.set_new_noise_schedule(\n",
    "    opt_model['model']['beta_schedule']['val'], \n",
    "    schedule_phase='val'\n",
    ")\n",
    "\n",
    "print(\"✓ 模型加载完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 N2N 结果的辅助函数\n",
    "\n",
    "def get_n2n_path(patient_id, patient_subid):\n",
    "    \"\"\"获取 Teacher N2N 结果路径\"\"\"\n",
    "    if TEACHER_N2N_ROOT is None:\n",
    "        return None\n",
    "    \n",
    "    pid_str = f\"{int(patient_id):08d}\"\n",
    "    psid_str = f\"{int(patient_subid):010d}\"\n",
    "    \n",
    "    return os.path.join(\n",
    "        TEACHER_N2N_ROOT,\n",
    "        pid_str,\n",
    "        psid_str,\n",
    "        \"random_0\",\n",
    "        f\"epoch{TEACHER_N2N_EPOCH}\",\n",
    "        \"pred_img.nii.gz\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_n2n_volume(patient_id, patient_subid, slice_indices):\n",
    "    \"\"\"\n",
    "    加载 Teacher N2N 结果\n",
    "    \n",
    "    Args:\n",
    "        patient_id: 患者 ID\n",
    "        patient_subid: 患者 SubID  \n",
    "        slice_indices: 需要的 slice 索引列表\n",
    "    \n",
    "    Returns:\n",
    "        n2n_slices: list of 2D arrays，或 None\n",
    "    \"\"\"\n",
    "    pred_path = get_n2n_path(patient_id, patient_subid)\n",
    "    if pred_path is None:\n",
    "        return None\n",
    "    \n",
    "    npy_path = pred_path.replace('.nii.gz', '.npy')\n",
    "    \n",
    "    # 加载数据\n",
    "    if os.path.exists(npy_path):\n",
    "        data = np.load(npy_path)\n",
    "    elif os.path.exists(pred_path):\n",
    "        data = nib.load(pred_path).get_fdata()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    n2n_slices = []\n",
    "    for s in slice_indices:\n",
    "        if s < data.shape[2]:\n",
    "            img = data[:, :, s].astype(np.float32)\n",
    "            \n",
    "            # 应用与训练相同的预处理\n",
    "            # HE\n",
    "            if val_set.histogram_equalization and val_set.bins is not None:\n",
    "                from data.ct_dataset import apply_histogram_equalization\n",
    "                img = apply_histogram_equalization(img, val_set.bins, val_set.bins_mapped)\n",
    "            \n",
    "            # HU clip + normalize to [0,1]\n",
    "            img = np.clip(img, HU_MIN, HU_MAX)\n",
    "            img = (img - HU_MIN) / (HU_MAX - HU_MIN)\n",
    "            \n",
    "            # 转回 HU (HE空间)\n",
    "            img_hu = img * (HU_MAX - HU_MIN) + HU_MIN\n",
    "            \n",
    "            # 逆向 HE\n",
    "            if use_inverse_he:\n",
    "                img_hu = inverse_histogram_equalization(img_hu, bins, bins_mapped)\n",
    "            \n",
    "            n2n_slices.append(img_hu)\n",
    "        else:\n",
    "            n2n_slices.append(np.zeros((data.shape[0], data.shape[1]), dtype=np.float32))\n",
    "    \n",
    "    return n2n_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理函数\n",
    "\n",
    "def inference_patient(patient_idx):\n",
    "    \"\"\"\n",
    "    对单个患者进行推理\n",
    "    \n",
    "    Returns:\n",
    "        dict: 包含 noisy, n2n, first, final volumes 和 patient info\n",
    "    \"\"\"\n",
    "    if patient_idx >= len(val_set.n2n_pairs):\n",
    "        return None\n",
    "    \n",
    "    pair = val_set.n2n_pairs[patient_idx]\n",
    "    patient_id = pair['patient_id']\n",
    "    patient_subid = pair['patient_subid']\n",
    "    \n",
    "    # 获取该患者的所有 samples\n",
    "    patient_samples = [(i, s) for i, s in enumerate(val_set.samples) if s[0] == patient_idx]\n",
    "    \n",
    "    if len(patient_samples) == 0:\n",
    "        return None\n",
    "    \n",
    "    noisy_results = []\n",
    "    first_results = []\n",
    "    final_results = []\n",
    "    slice_indices = []\n",
    "    \n",
    "    for sample_idx, (vol_idx, slice_idx) in tqdm(patient_samples, \n",
    "                                                  desc=f\"Patient {patient_idx}\", \n",
    "                                                  leave=False):\n",
    "        slice_indices.append(slice_idx)\n",
    "        sample = val_set[sample_idx]\n",
    "        \n",
    "        # 准备 batch\n",
    "        batch = {k: v.unsqueeze(0) if isinstance(v, torch.Tensor) else v \n",
    "                 for k, v in sample.items()}\n",
    "        \n",
    "        # 推理\n",
    "        diffusion.feed_data(batch)\n",
    "        diffusion.test(continous=True)\n",
    "        visuals = diffusion.get_current_visuals()\n",
    "        \n",
    "        # 提取结果\n",
    "        all_imgs = visuals['denoised'].numpy()\n",
    "        \n",
    "        # 从 [-1, 1] 转换到 [0, 1]\n",
    "        noisy = (all_imgs[0].squeeze() + 1) / 2\n",
    "        first = (all_imgs[1].squeeze() + 1) / 2\n",
    "        final = (all_imgs[-1].squeeze() + 1) / 2\n",
    "        \n",
    "        # 转换到 HU 值\n",
    "        noisy_hu = noisy * (HU_MAX - HU_MIN) + HU_MIN\n",
    "        first_hu = first * (HU_MAX - HU_MIN) + HU_MIN\n",
    "        final_hu = final * (HU_MAX - HU_MIN) + HU_MIN\n",
    "        \n",
    "        # 逆向 HE\n",
    "        if use_inverse_he:\n",
    "            noisy_hu = inverse_histogram_equalization(noisy_hu, bins, bins_mapped)\n",
    "            first_hu = inverse_histogram_equalization(first_hu, bins, bins_mapped)\n",
    "            final_hu = inverse_histogram_equalization(final_hu, bins, bins_mapped)\n",
    "        \n",
    "        noisy_results.append(noisy_hu)\n",
    "        first_results.append(first_hu)\n",
    "        final_results.append(final_hu)\n",
    "    \n",
    "    # 加载 N2N 结果\n",
    "    n2n_results = load_n2n_volume(patient_id, patient_subid, slice_indices)\n",
    "    \n",
    "    # 堆叠成 3D volume\n",
    "    result = {\n",
    "        'patient_id': patient_id,\n",
    "        'patient_subid': patient_subid,\n",
    "        'noisy': np.stack(noisy_results, axis=-1).astype(np.float32),\n",
    "        'first': np.stack(first_results, axis=-1).astype(np.float32),\n",
    "        'final': np.stack(final_results, axis=-1).astype(np.float32),\n",
    "    }\n",
    "    \n",
    "    if n2n_results is not None:\n",
    "        result['n2n'] = np.stack(n2n_results, axis=-1).astype(np.float32)\n",
    "    else:\n",
    "        result['n2n'] = None\n",
    "    \n",
    "    # 获取 affine\n",
    "    result['affine'] = np.eye(4)\n",
    "    noise_path = pair['noise_0']\n",
    "    if hasattr(val_set, '_fix_path'):\n",
    "        noise_path = val_set._fix_path(noise_path)\n",
    "    if os.path.exists(noise_path):\n",
    "        try:\n",
    "            result['affine'] = nib.load(noise_path).affine\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def save_results(result, output_dir):\n",
    "    \"\"\"\n",
    "    保存推理结果为 nii.gz 文件\n",
    "    \"\"\"\n",
    "    patient_id = result['patient_id']\n",
    "    patient_subid = result['patient_subid']\n",
    "    affine = result['affine']\n",
    "    \n",
    "    pid_str = f\"{int(patient_id):08d}\" if isinstance(patient_id, (int, float)) else str(patient_id)\n",
    "    psid_str = f\"{int(patient_subid):010d}\" if isinstance(patient_subid, (int, float)) else str(patient_subid)\n",
    "    \n",
    "    output_subdir = os.path.join(output_dir, pid_str, psid_str)\n",
    "    os.makedirs(output_subdir, exist_ok=True)\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    if SAVE_NOISY:\n",
    "        path = os.path.join(output_subdir, 'noisy_input.nii.gz')\n",
    "        nib.save(nib.Nifti1Image(result['noisy'], affine), path)\n",
    "        saved_files.append('noisy_input')\n",
    "    \n",
    "    if SAVE_N2N and result['n2n'] is not None:\n",
    "        path = os.path.join(output_subdir, 'n2n_teacher.nii.gz')\n",
    "        nib.save(nib.Nifti1Image(result['n2n'], affine), path)\n",
    "        saved_files.append('n2n_teacher')\n",
    "    \n",
    "    if SAVE_FIRST_STEP:\n",
    "        path = os.path.join(output_subdir, 'ddm2_first_step.nii.gz')\n",
    "        nib.save(nib.Nifti1Image(result['first'], affine), path)\n",
    "        saved_files.append('ddm2_first_step')\n",
    "    \n",
    "    if SAVE_FINAL:\n",
    "        path = os.path.join(output_subdir, 'ddm2_final.nii.gz')\n",
    "        nib.save(nib.Nifti1Image(result['final'], affine), path)\n",
    "        saved_files.append('ddm2_final')\n",
    "    \n",
    "    return saved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行推理\n",
    "print(f\"\\n开始推理 ({len(patient_indices)} 个患者)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "all_stats = []\n",
    "\n",
    "for patient_idx in tqdm(patient_indices, desc=\"Total Progress\"):\n",
    "    result = inference_patient(patient_idx)\n",
    "    \n",
    "    if result is None:\n",
    "        print(f\"  跳过 Patient {patient_idx} (无数据)\")\n",
    "        continue\n",
    "    \n",
    "    # 保存结果\n",
    "    saved = save_results(result, OUTPUT_DIR)\n",
    "    \n",
    "    # 记录统计信息\n",
    "    stats = {\n",
    "        'patient_id': result['patient_id'],\n",
    "        'shape': result['final'].shape,\n",
    "        'noisy_mean': result['noisy'].mean(),\n",
    "        'n2n_mean': result['n2n'].mean() if result['n2n'] is not None else None,\n",
    "        'first_mean': result['first'].mean(),\n",
    "        'final_mean': result['final'].mean(),\n",
    "    }\n",
    "    all_stats.append(stats)\n",
    "    \n",
    "    n2n_str = f\"{result['n2n'].mean():.0f}\" if result['n2n'] is not None else \"N/A\"\n",
    "    tqdm.write(f\"  ✓ Patient {result['patient_id']}: shape={result['final'].shape}, \"\n",
    "               f\"saved=[{', '.join(saved)}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ 推理完成!\")\n",
    "print(f\"  处理患者数: {len(all_stats)}\")\n",
    "print(f\"  输出目录: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计信息汇总\n",
    "if len(all_stats) > 0:\n",
    "    print(\"\\n统计信息汇总 (Mean HU)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Patient ID':<12} {'Shape':<18} {'Noisy':>10} {'N2N':>10} {'First':>10} {'Final':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for s in all_stats:\n",
    "        n2n_str = f\"{s['n2n_mean']:.1f}\" if s['n2n_mean'] is not None else \"N/A\"\n",
    "        print(f\"{str(s['patient_id']):<12} {str(s['shape']):<18} \"\n",
    "              f\"{s['noisy_mean']:>10.1f} {n2n_str:>10} \"\n",
    "              f\"{s['first_mean']:>10.1f} {s['final_mean']:>10.1f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n输出文件说明:\")\n",
    "    print(\"  - noisy_input.nii.gz    : 原始噪声CT输入\")\n",
    "    print(\"  - n2n_teacher.nii.gz    : Teacher N2N 去噪结果\")\n",
    "    print(\"  - ddm2_first_step.nii.gz: DDM² 第一步去噪 (粗去噪)\")\n",
    "    print(\"  - ddm2_final.nii.gz     : DDM² 最终去噪 (细去噪)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
